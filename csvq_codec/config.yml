# data
data_name: LIBRISPEECH
normalization: mean_var # min_max

# model
model_name: csvq_codec

#  ch_mult: 
#   - 4
#   - 4
#   - 8
#   - 8
#   - 16

# channel:
  # - 101
  # - 51
  # - 26
  # - 13
  # - 7

csvq_codec: 
  ch_mult: 
  - 4
  - 4
  - 8
  - 8
  - 16
  
  C_down0: 128
  C_down_rate: 0.2
  Groups: 6
  num_res_block: 6
  num_GRU_layers: 2
  codebook_size: 1024
  vq_commit: 1.0
  mel_factor: 0.1
  vq_factor: 0.25

  batch_size:
    train: 24
    test: 8
  shuffle:
    train: True
    test: False

  optimizer_name: Adam
  lr: 3.0e-4
  weight_decay: 0
  betas: [0.5,0.9]
  momentum: 0.9
  nesterov: True

  scheduler_name: ReduceLROnPlateau
  factor: 0.85
  patience: 1
  threshold: 1.0e-4
  min_lr: 1.0e-8
  
  num_epochs: 40
  
# dataset
stats:
  LIBRISPEECH:
    - [-0.00031031499383971095, -3.4487189992660205e-08]
    - [0.5290760397911072, 0.4847901165485382]
  LIBRISPEECH_SMALL:
    -
    -
raw_F: 201

# experiment
num_workers: 0
init_seed: 0
num_experiments: 1
device: cuda
world_size: 1
resume_mode: 0
verbose: False
pivot_metric: PESQ
cuda_device: 2
plot_interval: 0.5

sr: 16000
win_length: 20
hop_length: 5

train_dur: 3
test_dur: 10

train_clips: 180000
test_clips: 1000

save_image: True

fixed_bitstream: 7









